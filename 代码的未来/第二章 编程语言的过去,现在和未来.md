# 第二章 编程语言的过去,现在和未来

---

## 编程语言的世界

* FORTRAN
* COBOL
* LISP
* SNOBOL

### 以数学为基础的编程语言

函数型编程语言.

`ML`,`Haskell`.

倾向于直接表达要表达的概念. 相对于`How`(如何实现), 更倾向于通过`What`(想要什么)来表单问题. 这样**可以不被机器的处理方式所左右,将问题抽象地表达出来**.

### 抽象化

从编程语言的进化过程来看, 一个显著的关键词就是**抽象化**.

抽象化就是提供一个抽象的概念, 使用者即便不具备关于内部详细情况的知识, 也能够对其进行运用. 由于不必了解其内部的情况, 因此也被称为**黑箱化**.

人类一次所能掌握的概念数量是有限的, 如果能够让问题的处理方式更加抽象, 也就可以解决更复杂的问题.

### 20年后的语言

20年后的语言, 应该是在分布处理(多台计算机协作处理)和并行处理(多个CPU协作处理)功能的强化.

## DSL(特定领域语言)

`DSL`是指利用特定领域(`Domain`)所专门设计的词汇和语法, 简化程序设计过程, 提高生产效率的技术, 同时也让非编程领域专家的人直接描述逻辑称为可能.

优点: 可以直接使用其对象领域中的概念, 集中描述"想要做到什么(What)"的部分, 而不必对"如何做到(How)"进行描述.

### 领域特定语言

* `awk`
* `Makefile`
* `sed`
* `yacc`语法分析器生成工具
* `lex`词法分析器生成工具

### 外部DSL

`Unix`文化中, 由若干单一目的的小工具所组成的"工具箱系统". 各种迷你语言, 作为组成工具箱的零部件.

* XML
* SQL
* 正则表达式, 用来描述字符串模板的一种外部DSL, 其语法在不同的语言中基本上是通用的.

独立于程序开发的语言. 对于某个领域进行操作的程序, 不一定是用同一种语言来编写的.

外部`DSL`实际上是全新设计的语言和语言引擎, 因此可以根据它的目的进行自由的设计, 不必被特定的执行模块和现有语言的语法所左右. 由于自由度很高, 在特定领域中能够大幅度提高生产效率.

### 内部DSL

**不是**创造一种新的语言, 而是在现有语言中实现`DSL`, 而作为`DSL`基础的这种现有语言, 称为宿主语言.

`Lisp`,`Smalltalk`,`Ruby`适合.

内部`DSL`是"借宿"在宿主语言中的, 它借用了宿主语言的语法, 因此程序员无需学习一种新的语言.

### DSL的优势

提供生产效率.

* 为特定领域所设计的词汇,
* 可以在高级层面上编写程序.

不涉及对象领域的内部细节, 而是在高级层面上进行描述, 这就是近半个世纪依赖编程语言进行的方向--"抽象化".


### 流畅接口

流畅接口运用了方法链(Method chain).

```
cusotmer.newOrder().with(6, "Tal")
				    .with(5, "HPK").skippable()
				    .with(3, "LBV")
				    .priorituRush()
				    
				    
如果不使用流畅接口

Order o1 = new Order();
customer.addOrder(o1);

OrderLine line1 = new OrderLine(6, Product.find("TAL"));
o1.addLine(line1);

OrderLine line2 = new OrderLine(5, Product.find("HPK")).
line2.setSkippable(true);
o1.addLine(line2);
...
```


简洁性和易读性使用流畅接口会较好, 但是现在还没有普及.

### 外部DSL 示例

`YAML`和`JSON`就是为了**将对象用对人类易读的形式描述出来**这一特定目的而设计的外部`DSL`.

### DSL 设计的构成要素

* 上下文(Context)
* 语句(Sentence)
* 单位(Unit)
* 词汇(Vocabulary)
* 层次结构(Hierarchy)

## 元编程

**元编程**, 就是用程序来编写程序.

程序本身的信息是可以被访问的, 因此在程序运行过程中也可以对程序本身进行操作, 这就是元编程.

元编程的灵活性:

比如`ActiveRecord`模式, 通过元编程技术对无法预先确定的操作进行了应对, 这样一来, 未来的可能性就不会被禁锢, 体现了语言的灵活性.

## 内存管理

### 垃圾

就是需要回收的对象. 如果程序(通过某个变量等等)可能会直接或间接地引用一个对象, 那么这个对象就被视为"存活". 与之相反, 已经引用不到的对象被视为"死亡". 将这些"死亡"对象找出来, 然后作为垃圾进行回收, 就是**GC的本质**

### 根

根(Root), 就是判断对象是否可被引用的起始点. 基本是将**变量**和**运行栈空间**作为根.

### 标记清除方式

`Mark and Sweep`标记清除. 

* 首先从根开始将可能被引用的对象用递归的方式进行标记
* 将没有标记到的对象作为垃圾进行回收

清除阶段: 将全部对象按顺序扫描一遍, 将没有被标记的对象进行回收. 在扫描的同时, 还需要将存活对象的标记清除掉, 以便为下一次`GC`操作做好准备.

作为标记清除的变形, 还有一种叫做标记压缩(Mark and Compact)的算法, 它不是将被标记的对象清除, 而是将它们不断压缩.

#### 缺点

在分配了大量对象, 并且其中只有一小部分存活的情况下, 所消耗的时间会大大超过必要的值, 这是因为在清除阶段还需要对大量死亡对象进行扫描.

### 复制收集方式

`Copy and Collection`

将从根开始被引用的对象复制到另外的空间中, 然后, 再将复制的对象所能够引用的对象用递归的方式不断复制下去. 最后将旧空间废弃掉, 这样就可以将死亡对象所占用的空间一口气全部释放出来, 而没有必要再次扫描每个对象. 下次`GC`的时候, 现在的新空间也就变成了将来的旧空间.

#### 局部性

在复制收集过程中, 会按照对象被引用的顺序将对象复制到新空间中. 于是, 关系较近的对象被放在距离较近的内存空间中的可能性会提高, 这被称为"局部性". 局部性高的情况下, 内存缓存会更容易有效运作, 程序的运行性能也能够得到提高.

### 引用计数方式

`Reference Count`

在每个对象保存该对象引用计数, 当引用发生增减时对计数进行更新.

引用计数的增减, 一般发生在:

* 变量赋值
* 对象内容鞥新
* 函数结束(局部变量不再被引用)

一个对象的引用计数变为0时, 则说明它将来不会再被引用, 因此可以释放相应的内存空间.

释放操作是针对每个对象个别执行的, 因此和其他算法相比, 由`GC`而产生的中断时间(`Pause time`)就比较短.

#### 缺点

* 无法释放循环引用的对象.
* 必须在应用发生增减时对引用计数做出正确的增减.
* 不适合并行处理. 如果多个线程同时对引用计数进行增减的话, 引用计数的值就可能会产生不一致的问题. 为了避免这种情况的发生, 对引用计数的操作必须采用独占的方式来进行. 如果引用操作频繁发生, 每次都是用加锁等并发控制机制的话, 开销较大.

为了避免循环引用的问题, 都配合使用了其他的`GC`机制.

### 分代回收




