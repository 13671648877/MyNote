# 27_03_nfs服务

---

### 笔记

---

#### NFS

Network File System

文件系统工作在内核空间.

文件系统是以模块工作在内核空间.

任何程序可以执行的命令都工作在用户空间,比如格式化`mke2fs`是工作在用户空间的文件系统管理工具.

`NFS`工作在内核空间.

`Sun`开发的文件系统.

* `NFSv2`
* `NFSv3`
* `NFSv4`

只能工作在`Uninx/Linux`主机之间.

类似`Windows`的网络邻居.

**文件系统**

磁盘空间存储数据,为了方便有效管理,将磁盘空间映射为一个软件结构.

内核里面如`ext3`,在磁盘空间划分为:

* 元数据空间
	* 非文件内容相关信息
	* 用于管理磁盘分区
* 数据空间

数据空间会划分成存储数据的磁盘块, 但是该磁盘块不会真正划分到磁盘上,而是由`元数据空间`维护的一张映射表.

**nfs**

可以通过网络来工作.

一个主机的目录`/mydata/data`挂载一个设备,而该设备不在本机上,在另外一台主机上.即挂载远程主机的设备.

`/mydata/data` --> REMOTEHOST:/var/ftp

用户的应用程序往该目录写数据时,会通过网络发送到另外一台主机,另外一台主机会写入自己磁盘.

客户端像访问本地设备一样访问远程设备.

**文件系统写数据**

用户往文件系统写数据,能够实现对硬件操作的只有内核.内核对硬件的操作功能是通过==系统调用==输出的.

* read() 内核函数,函数调用,过程调用
* write() 内核函数,函数调用,过程调用

本地的应用程序实现一些操作都是通过==本地过程调用(local procedure call)==实现的,

**nfs实现**

文件系统自身没有网络功能. (`mkdir`,`touch`没有网络功能)

两台主机上, 客户端和服务器端建立关系,但是客户端并==不能意识到是工作在网络上的==.

客户端 和 服务端 只需要考虑是工作在本地的.

用`rpc`(remote procedure call 远程过程调用)实现.

当客户端发起一个系统调用的时候, 没有直接发给内核, 程序自身所工作的主机借助客户端程序发起一个特殊的系统调用`rpc`客户端`stub`(存根调用),

服务器端需要运行一个服务器进程,`rpc server`.

过程:

1. 当内核发现文件系统请求的设备不是本地设置, 于是借助`stub`(`rpc`客户端)将请求转给`rpc`客户端.
2. `rpc`客户端请求`rpc server`
3. `rpc server`获取到命令后转给`本机文件系统`在本地完成用户的请求,并且将处理结果返回给`rpc 客户端`
4. `rpc 客户端`在返回给用户

过程是==透明的==,但是需要借助`rpc`在中间调用.网络通信过程==隐藏在==`rpc`背后.

#### rpc

Linux 提供`rpc`服务的程序, `Portmap`: `111`(端口)/tcp, `111`(端口)/udp.

`RPC` 是协议, `Portmap` 是实现. `RPC` 也可以基于`HTTP`实现(传输`xml`格式报文).

`RPC` 是一种编程技术,协议, 主要是用于实现简化分布式应用程序开发, 协调两个主机的进程的网路通信.`RPC`有客户端和服务器端, 它能链接两个主机, 将两个主机连起来, 让两个主机的进程看起来像是本地通信一样.

`C` -> `RPC 的客户端` --> `RPC 服务器端`通信 -> `S` 真正的程序.

**rpc 数据传输格式**

* 二进制
* 文本格式
	* xmlrpc: 基于`HTTP`实现(传输`xml`格式报文) -> `SOAP`(简单对象访问协议)轻量级的`xmlrpc`.

**rpc 链接方式第一种**

`app(客户端应用程序)` -> `stub` -> `rpc server`-> `文件系统`(通过文件系统监听的端口号, 端口号告知给`rpc`, 当`rpc server`收到报文后会转给该端口号).

`服务器的文件系统`也需要监听一个随机端口号(告诉给`rpc`), `rpc server`接收到`stub`的封装后的网络报文后在将网络报文转交给`服务器文件系统监听的随机端口号`, 不直接和客户端通信.

**rpc 链接方式第二种**

`nfs`使用该种模式.

初始化链接由`rpc`建立, 随后`stub`直接给工作在网络层的`文件系统`通信.

`nfs`监听在某个端口上, `stub`链接`rpc server`(`111`端口), `rpc server`把对应的`rpc 服务`所使用的端口号(`nfs`监听的端口号)返回给客户端, 客户端在自己链接`nfs`监听的端口号.

#### nfs 文件保存属主

不验证客户端用户名, 只验证`ip`地址.

**服务器端没有对应用户**

```shell
客户端:
jerry:1000(UID)

服务器端:
对应保存 1000(UID)
```

**服务器端有对应用户**

```shell
客户端:
jerry:1000(UID)

服务器端: 假设 tom 是uid 1000 的用户
对应保存 tom:1000(UID)
```

#### nfs lock

防止同时写同一个文件.

举例: 在一台主机用`vim`同时打开两个文件, 第二次不让打开. 
 
#### nfs 软件包

`nfs-utils`.

```shell
[root@iZ944l0t308Z ~]# yum install nfs-utils
...

可以看见相应的 rpc 包
[root@iZ944l0t308Z ~]# rpm -ql nfs-utils
...
/usr/sbin/rpc.gssd
/usr/sbin/rpc.idmapd
/usr/sbin/rpc.mountd
/usr/sbin/rpc.nfsd
/usr/sbin/rpc.svcgssd
...
```

**启动`nfs`服务**

```shell
[root@iZ944l0t308Z ~]# service nfs start
Redirecting to /bin/systemctl start  nfs.service
[root@iZ944l0t308Z ~]# service nfs status
Redirecting to /bin/systemctl status  nfs.service
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)
   Active: active (exited) since Sun 2017-07-09 12:46:33 CST; 3s ago
  Process: 31070 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)
  Process: 31068 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS)
 Main PID: 31070 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service

Jul 09 12:46:33 iZ944l0t308Z systemd[1]: Starting NFS server and services...
Jul 09 12:46:33 iZ944l0t308Z systemd[1]: Started NFS server and services.
```

`nfs`启动三个进程:

* `nfsd`: nfs服务
* `mountd`: 挂载
* `quotad`: 磁盘配额

`nfsd`监听 `2049/tcp`, `2049/udp`

`mountd`: 端口不定,启动向`rpc`服务器注册.最好事先定义好端口,避免占用常用端口号(80,3306...)

`quotad`: 端口不定,启动向`rpc`服务器注册.最好事先定义好端口,避免占用常用端口号(80,3306...)

**查看`rpc`本地服务**

```shell
[root@iZ944l0t308Z ~]# rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  47451  status
    100024    1   tcp  40871  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  35805  nlockmgr
    100021    3   udp  35805  nlockmgr
    100021    4   udp  35805  nlockmgr
    100021    1   tcp  47777  nlockmgr
    100021    3   tcp  47777  nlockmgr
    100021    4   tcp  47777  nlockmgr
```

#### nfs 配置文件

`/etc/exports`:

* 每一行包含一个共享出去的文件系统, 以及哪些客户端可以访问此文件系统. 客户端有多个, 使用空白字符分隔.
* 每一个客户端后面必须要跟一个小括号, 里面定义了此客户端的访问特性.

		172.16.0.0/16(ro,async) 只读,异步	192.16.0.0./24(rw,sync) 读写,同步
	
* 以`#`开头的信息都是注释行.

**客户端**

* 单个主机
	* FQDN
	* IP
* 网络组
* 通配符
	* `*.example.com`
* IP 网络 (address/netmaks)

#### nfs 示例

共享 `/shared` 目录.

```shell
120.25.87.35 主机

[root@iZ944l0t308Z ~]# mkdir /shared
[root@iZ944l0t308Z ~]# vim /etc/exports
[root@iZ944l0t308Z ~]# cat /etc/exports
/shared 	120.25.161.1(ro)

重启服务
[root@iZ944l0t308Z ~]# service nfs restart
Redirecting to /bin/systemctl restart  nfs.service

[root@iZ944l0t308Z ~]# showmount -e 120.25.87.35
Export list for 120.25.87.35:
/shared 120.25.161.1
```

登录客户端主机

```shell
ssh ansible@120.25.161.1 -p 17456

也可以使用 showmount 命令
[ansible@rancher-agent-2 ~]$ showmount -e 120.25.87.35
Export list for 120.25.87.35:
/shared 120.25.161.1

客户端可以直接挂载文件系统,我们挂载至本地/mnt/nfs下

[ansible@rancher-agent-2 ~]$ sudo mkdir /mnt/nfs
[ansible@rancher-agent-2 ~]$ sudo mount -t nfs 120.25.87.35:/shared /mnt/nfs
```

测试

```shell
120.25.87.35

创建一个文件,检测是否可以共享到客户端.

[root@iZ944l0t308Z ~]# cd /shared/
[root@iZ944l0t308Z shared]# ls
[root@iZ944l0t308Z shared]# echo 111 > testnfs
```

```shell
120.25.161.1

[ansible@rancher-agent-2 nfs]$ ls
testnfs
[ansible@rancher-agent-2 nfs]$ cat testnfs
111

RO 只读, 不能写
[ansible@rancher-agent-2 nfs]$ touch 2
touch: cannot touch '2': Read-only file system
```

**`showmount`**

* `-a NFS_SERVER`: 列出所有的客户端名称或地址 和 挂载的目录(*我在centos7 下命令没有效果?*).
* `-e NFS_SERVER`: 显示服务器共享了哪些目录.
* `-d NFS_SERVER`: 显示NFS服务器所有倒出的文件系统中被客户端挂在了文件系统列表.

**客户端挂载`nfs`文件系统**

`mount -t nfs NFS_SERVER:/挂载路径 /本地挂载点`

**`exportfs`**

`exportfs`命令用来管理当前NFS共享的文件系统列表.

我们更新了`/etc/exports`, 但是假如每次都要重启服务, 万一重启服务的时候, 共享目录正在传输数据怎么办?

所以我们使用`exportfs`来重新加载配置文件.

参数:

* `-a`: 根`-r`或`-u`选项同时使用, 表示重新挂载所有文件系统或取消倒出所有文件系统.
* `-r`: 重新共享所有目录
* `-u`: 取消一个或多个目录的共享
* `-v`: 显示详细信息

示例:

```shell
我们添加一条记录在 /etc/exports 中
[root@iZ944l0t308Z ~]# cat /etc/exports
/shared 	120.25.161.1(ro)
/var/ftp	120.25.161.1(ro)

登录客户端,我们看不见新添加的 /var/ftp
[ansible@rancher-agent-2 ~]$ sudo showmount -e 120.25.87.35
Export list for 120.25.87.35:
/shared 120.25.161.1

登录服务端执行 exportfs
[root@iZ944l0t308Z ~]# exportfs -rav
exporting 120.25.161.1:/var/ftp
exporting 120.25.161.1:/shared

客户端可以看见
[ansible@rancher-agent-2 ~]$ sudo showmount -e 120.25.87.35
Export list for 120.25.87.35:
/var/ftp 120.25.161.1
/shared  120.25.161.1
```

#### nfs 文件系统导出属性

* ro: 只读
* rw: 读写
* sync: 同步(性能差)
* async: 异步
* root_squash
* no_root_squash
* all_squash: 无论是谁都转换成来宾账户
* anonuid: 指定哪个uid为来宾用户
* anongid: 指定哪个gid为来宾用户组

**nfs 的 `root` 用户**

默认`root`用户不允许的.

配置项 `root_squash`, 将`root`用户映射为来宾账户.

`no_root_squash` 保留`root`用户权限(客户端主机在共享目录以`root`用户可以类似服务端用户`root`一样操作)

**示例**

把所用用户都映射为`id=511`的用户.

服务器端:

```shell
120.25.87.35

[root@iZ944l0t308Z ~]# useradd -u 510 nfstest
[root@iZ944l0t308Z ~]# touch /shared/nfstest
[root@iZ944l0t308Z ~]# chown nfstest:nfstest /shared/nfstest
[root@iZ944l0t308Z ~]# vim /etc/exports
[root@iZ944l0t308Z ~]# cat /etc/exports
/shared 	120.25.161.1(rw,all_squash,anonuid=510,anongid=510)
/var/ftp	120.25.161.1(ro)
[root@iZ944l0t308Z ~]# exportfs -ra
```

客户端:

```shell
120.25.161.1

[ansible@rancher-agent-2 ~]$ sudo umount /mnt/nfs
重新挂载
[ansible@rancher-agent-2 ~]$ sudo !mount
sudo mount -t nfs 120.25.87.35:/shared /mnt/nfs
```

#### nfs 开机自动挂载

```shell
/etc/fstab
...
120.25.87.35:/shared	/mnt/nfs	nfs	defaults,_rnetdev 0 0
```

`netdev`, 网络设备如果挂不上则忽略到, 否则系统挂载不到则会不能开机.

#### nfs 认证

`rpc.mountd` 就是 `nfs` 的 `mountd` 进程.

`rpc.mountd` 实现认证, 如果是允许访问的则返回令牌.

1. 与rpc通信, rpc返回mountd端口
2. 通过mountd验证, 如果验证通过返回令牌
3. 用令牌和`nfs`通信

nfs 配置文件`/etc/sysconfig/nfs`

可以修改`mountd`启动端口号

### 整理知识点

---

#### nfs 服务器端挂掉, 客户端卡死

我第一次在客户端挂载目录, 当服务器关闭`nfs`服务后, 客户端执行`ls /mnt/nfs`后会卡死.

需要在客户端挂载时候使用`soft`选项.

```shell
mount -t nfs -o soft,timeo=5,retry=3 120.25.87.35:/shared /mnt/nfs

服务器端关闭nfs后, 不会卡死, 会返回错误  Input/output error
[ansible@rancher-agent-2 mnt]$ ls /mnt/nfs/
ls: cannot access /mnt/nfs/: Input/output error
```